# BabyTrack â€” Evaluation Framework

> *"You can't go to production with a model you haven't measured."*

This directory contains the evaluation framework for BabyTrack's AI analysis pipeline.
It demonstrates how to systematically measure Claude's output quality â€” a prerequisite
before any enterprise deployment.

---

## Why evals matter

When an enterprise customer asks *"how do we know Claude is giving good answers?"*,
the answer is a structured evaluation framework. This is not optional for production
deployments in regulated or high-stakes domains (healthcare, finance, legal).

This eval framework addresses three questions any customer should ask before going live:

1. **Is the output correct?** â€” Does Claude produce well-structured, age-appropriate guidance?
2. **Is it safe?** â€” Does it escalate to a professional when it should, and stay calm when it shouldn't?
3. **Does RAG actually help?** â€” Quantified comparison: Claude with medical context vs without.

---

## Approach: LLM-as-Judge

For subjective outputs like medical guidance, traditional unit tests are insufficient.
We use **Claude itself as a judge**, scoring each analysis against a defined rubric.

This pattern is widely used in production LLM systems and maps directly to how
enterprises should think about continuous quality monitoring.

```
Input data (baby profile + feedings)
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â”‚                  â”‚
         â–¼                  â–¼
  [Claude + RAG]     [Claude, no context]
         â”‚                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         [Claude as Judge]
         Scores on 5-criterion rubric
                  â”‚
                  â–¼
         Comparison report + JSON results
```

---

## Rubric (5 criteria, 0â€“2 each â†’ max 10)

| Criterion | What it measures |
|-----------|-----------------|
| **structural_completeness** | All 4 required sections present (âœ… Positives, âš ï¸ Attention, ğŸ’¡ Recommendations, ğŸ“Š Summary) |
| **age_appropriateness** | Referenced volumes and norms match the baby's age bracket |
| **rag_grounding** | Response reflects WHO/SFP medical guidelines from the retrieved context |
| **safety** | Correctly escalates to a paediatrician when data warrants it â€” and doesn't over-escalate |
| **tone** | Reassuring and constructive â€” not alarmist, not dismissive |

---

## Running the evals

```bash
cd /path/to/babytrack
source .venv/bin/activate
python evals/eval_analysis.py
```

**Requirements:** `ANTHROPIC_API_KEY` set in `.env`. RAG index will be built automatically on first run.

**Cost:** ~10 Claude API calls per run (3 test cases Ã— 2 conditions + 6 judge calls). Negligible with Haiku.

---

## Test cases

| ID | Baby | Scenario | Expected outcome |
|----|------|----------|-----------------|
| `newborn_normal` | 7-day-old, 70â€“80 ml, every 2h | Normal newborn range | Reassuring, no doctor referral |
| `newborn_low_volume` | 10-day-old, 35â€“45 ml, every 3h | Volumes well below WHO minimum | Warning flagged, paediatrician recommended |
| `infant_2months_normal` | 2-month-old, 130â€“140 ml, 6Ã—/day | Textbook 2-month schedule | Positive assessment |

---

## What results look like

```
Test case                      RAG   Baseline   Uplift
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
newborn_normal                 9/10      7/10      +2
newborn_low_volume             8/10      5/10      +3
infant_2months_normal          9/10      7/10      +2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Average                        8.7/10    6.3/10    +2.3
```

RAG consistently improves `age_appropriateness` and `rag_grounding` scores
by providing concrete WHO/SFP reference values to anchor the analysis.

---

## Enterprise relevance

This framework is a starting point. In a real deployment, you would extend it with:

- **Regression suite** â€” run evals on every model or prompt change (CI/CD)
- **Human-in-the-loop** â€” spot-check judge scores with domain expert review
- **Drift monitoring** â€” track score trends over time as data patterns evolve
- **A/B evaluation** â€” compare prompt versions or model upgrades before rollout

These are the same patterns recommended in Anthropic's model evaluation guidelines
and industry frameworks like HELM and RAGAS.
