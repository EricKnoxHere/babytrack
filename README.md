# ðŸ¼ BabyTrack

> **Portfolio project â€” Solutions Architect @ Anthropic**
> Demonstrating how RAG + Claude can be deployed responsibly in a regulated, high-stakes domain.

---

## The idea

A paediatrician has limited time. A new parent has infinite anxiety. There's a gap.

BabyTrack closes it: an infant feeding tracker that doesn't just store data, but analyses it â€” grounded in WHO and SFP medical guidelines â€” and surfaces personalised, actionable recommendations via Claude.

The real point isn't the app. It's what building it required me to think about:

- How do you ground an LLM in a specific knowledge base without hallucination risk?
- How do you measure whether the grounding actually helps?
- How do you build an API that an enterprise team could extend, not just a demo that runs on localhost?

---

## What it does

| Feature | What you can do |
|---------|-----------------|
| **ðŸ“ Feeding log** | Record bottle/breastfeeding with timestamps, volumes, notes. Edit or delete any entry. |
| **ðŸ©² Diaper tracking** | Log diaper changes (wet, soiled, mixed) with timestamps and notes. |
| **âš–ï¸ Weight tracking** | Log growth checkpoints. View historical data and trends. |
| **ðŸ’¬ AI chat** | Conversational interface to ask questions about your baby's patterns, grounded in medical guidelines via RAG. |
| **ðŸ“Š Analytics** | Visualise 7â€“30 day feeding patterns: volume trends, frequency, type breakdown. |
| **ðŸ“¥ CSV export** | Download all feeding data for external analysis or sharing. |
| **ðŸ¤– AI analysis** | Claude-powered recommendations grounded in WHO/SFP guidelines via RAG. Shows which medical documents were cited. |
| **âœ… Quality evaluation** | LLM-as-judge framework scores output quality. Demonstrates RAG value vs baseline. |
| **ðŸ”Œ REST API** | Full CRUD on babies, feedings, weights, diapers, conversations. OpenAPI auto-docs. Production-ready async architecture. |

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       STREAMLIT UI                          â”‚
â”‚  Dashboard Â· Feeding entry Â· AI Analysis                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FASTAPI (main.py)                        â”‚
â”‚  POST /babies   POST /feedings   GET /analysis/{id}        â”‚
â”‚  GET  /babies   GET  /feedings   GET /health               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                         â”‚
            â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SQLite           â”‚     â”‚         RAG PIPELINE               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚     â”‚                                   â”‚
â”‚  babies           â”‚     â”‚  data/docs/                       â”‚
â”‚  feedings         â”‚     â”‚  â””â”€â”€ sfp_guide_alimentation_      â”‚
â”‚  diapers          â”‚     â”‚      nourrisson.md                â”‚
â”‚  weights          â”‚     â”‚                                   â”‚
â”‚  conversations    â”‚     â”‚                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                                   â”‚
                          â”‚           â”‚                        â”‚
                          â”‚    LlamaIndex VectorStoreIndex     â”‚
                          â”‚    (BAAI/bge-small-en-v1.5)       â”‚
                          â”‚           â”‚  top-k chunks          â”‚
                          â”‚           â–¼                        â”‚
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
                          â”‚  â”‚  Claude Haiku        â”‚          â”‚
                          â”‚  â”‚  (Anthropic API)     â”‚          â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why RAG here?

A generic LLM knows roughly what WHO recommendations say. A RAG-grounded LLM cites the *specific thresholds and intervals* from the actual guidelines â€” the difference between "drink more water" and "a 7-day-old should receive 60â€“90 ml per feed every 2â€“3 hours."

In a regulated domain (medical, legal, financial), that precision gap is the entire value proposition of RAG.

---

## Evaluation framework

Before shipping any LLM feature to production, the right question is: *how do you know it's giving good answers?*

The `evals/` folder answers this. An LLM-as-judge script runs 3 clinical scenarios (healthy newborn, low-intake alert, mixed feeding) and scores each Claude response across 5 criteria, with and without RAG context:

| Criterion | What it checks |
|-----------|---------------|
| `age_appropriate` | References norms specific to the baby's age |
| `rag_grounded` | Reflects retrieved WHO/SFP guidelines, not generic knowledge |
| `actionable` | Recommendations are concrete and immediately usable |
| `safety_flag` | Correctly raises or withholds a clinical concern |
| `tone` | Reassuring when warranted; appropriately concerned when not |

```bash
python evals/eval_analysis.py
# â†’ Scores per scenario, RAG vs baseline delta, saved to evals/results/
```

Results from a sample run:

```
Average score â€” RAG: 15.0/15  |  Baseline: 14.7/15
RAG improvement: +0.3 points across scenarios
Sections present: 4/4 on all runs
```

The eval framework is as important as the application itself â€” it's the scaffolding you'd build with any enterprise customer before a production go-live.

> See `evals/README.md` for the full methodology and how to extend it.

---

## Enterprise considerations

This is a portfolio demo, but the architecture decisions reflect real deployment constraints:

| Concern | Decision made | Enterprise path |
|---------|--------------|-----------------|
| **Data isolation** | SQLite per deployment | PostgreSQL + one schema per tenant for multi-tenancy |
| **Hallucination risk** | RAG-grounded prompts + structural output format | Eval suite + human review for high-stakes outputs |
| **Observability** | Structured logging, token counts captured | Feed into Datadog / CloudWatch |
| **Auth** | Not implemented | Add OAuth2 / SSO at the API gateway layer |
| **Index freshness** | Manual rebuild | Trigger on document update via webhook |
| **Cost control** | Haiku model, 1024 max tokens | Budget alerts + model tiering by use case |

---

## Changelog highlights

- âœ… **Full CRUD for feedings** â€” create, read, update, delete with inline edit forms
- âœ… **Diaper tracking** â€” log diaper changes (wet, soiled, mixed) with timestamps
- âœ… **Weight tracking** â€” record growth checkpoints, view history
- âœ… **AI chat** â€” conversational interface grounded in medical guidelines via RAG
- âœ… **CSV import/export** â€” import data from CSV, download feeding data for analysis
- âœ… **RAG source attribution** â€” see which medical guideline each recommendation comes from
- âœ… **Streamlit UI** â€” custom theme, responsive layout, mobile-optimised sidebar
- âœ… **Eval framework** â€” LLM-as-judge scoring on 5 criteria; RAG vs baseline comparison
- âœ… **Self-hosted deployment** â€” ngrok tunnel with static domain for remote access

---

## Running locally

```bash
# 1. Install & activate
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2. API key
cp .env.example .env
# Edit .env â†’ add ANTHROPIC_API_KEY

# 3. Start services
uvicorn main:app --reload
# (in another terminal)
streamlit run ui/app.py
```

Open UI: **http://localhost:8501**  
API docs: **http://localhost:8000/docs**

---

## Tests

```bash
pytest tests/ -v
# 151 tests Â· 0 failures Â· zero network calls
```

| Suite | Tests | What's covered |
|-------|-------|---------------|
| FastAPI endpoints | 44 | Babies, feedings, diapers, weights, analysis, conversations |
| RAG pipeline | 34 | Indexer, retriever, analyzer (MockEmbedding + mock Anthropic) |
| Diapers | 18 | CRUD, filtering by day/range, cascade deletes |
| Feedings | 15 | CRUD, filtering by day/range, cascade deletes |
| Conversations | 13 | Create, list, messages, context management |
| Weights | 9 | Add, get, update, delete, range queries |
| Reports | 9 | Daily/weekly report generation and caching |
| Babies | 9 | CRUD, cascade deletes, validation |

---

## Project structure

```
babytrack/
â”œâ”€â”€ main.py                  # FastAPI entry point + lifespan
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ models/              # Pydantic v2 â€” Baby, Feeding, Diaper, Weight
â”‚   â”œâ”€â”€ services/            # Async CRUD (aiosqlite)
â”‚   â”œâ”€â”€ rag/                 # LlamaIndex â€” indexer, retriever, analyzer
â”‚   â””â”€â”€ api/routes/          # babies, feedings, diapers, weights, analysis, chat
â”œâ”€â”€ evals/                   # LLM-as-judge eval framework
â”‚   â”œâ”€â”€ eval_analysis.py     # 3 scenarios Â· 5 criteria Â· RAG vs baseline
â”‚   â””â”€â”€ results/             # JSON results per run (gitignored)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start_tunnel.sh      # Launch API + UI + ngrok tunnel
â”‚   â””â”€â”€ import_*.py          # CSV data import utilities
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ app.py               # Streamlit dashboard
â”‚   â”œâ”€â”€ api_client.py        # HTTP wrapper
â”‚   â””â”€â”€ views/               # Home, Record, Chat pages
â””â”€â”€ data/
    â”œâ”€â”€ docs/                # SFP medical guidelines (markdown)
    â””â”€â”€ index/               # Persisted vector index (gitignored)
```

---

## Key technical decisions

| Decision | Rationale |
|----------|-----------|
| **FastAPI** | Native async, auto OpenAPI, Pydantic validation â€” what most enterprise Python teams are standardising on |
| **SQLite + aiosqlite** | Zero-config, portable, self-contained â€” ideal for self-hosted deployment |
| **LlamaIndex** | Mature RAG abstraction with index persistence â€” not reinventing retrieval |
| **BAAI/bge-small-en-v1.5** | 130 MB, runs offline, multilingual â€” no embedding API dependency |
| **Structured prompt output** | Fixed markdown sections make parsing and eval deterministic |
| **LLM-as-judge** | Industry-standard pattern for scalable output evaluation without human labellers |

---

## Self-hosted deployment (ngrok)

The app runs locally and is exposed via an ngrok tunnel with a static domain:

```bash
# One command to start API + UI + tunnel
./scripts/start_tunnel.sh
# â†’ https://<your-domain>.ngrok-free.dev
```

Requires a free [ngrok account](https://ngrok.com/) with a static domain configured. Set `NGROK_AUTHTOKEN` and `NGROK_DOMAIN` in `.env`.

---

*Built as part of an SA Applied AI portfolio â€” Anthropic Paris.*
*The goal was not to build a baby app. The goal was to build something that shows how I think about RAG architecture, eval, and production deployment.*
